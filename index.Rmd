---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Seungho Lee; SL46699

### Introduction 

Paragraph or two introducing your datasets and variables, why they are interesting to you, etc. See instructions for more information

*Continuing with the theme from my project 1, I have decided to utilize the same nbastats dataset that was obtained from kaggle for my project. However, for the purpose of this project, this list of players have been narrowed down to the best 50 players (50 observations) from the 2018-2019 NBA season. Additionally, a new variable was added called "citizenship" which will be the binary variable for the dataset, and the I have also altered the stat variables from the original dataset in order to only work with the stats/variables that I was particularly interested in. The nbastats dataset includes character variables such as name, height, weight, team, and citizenship. The dataset also involves variables that measure numerical values such as age, salary, points, blocks, steals, assists, rebounds, free throw percentage, 3 point field goal percentage, field goal percentage, minutes played, and games played. For my binary variable ("citizenship"), there are 35 observations for USA and there are 15 observations for International. Also, the data did not need any tidying. I chose this particular dataset and these specific variables because growing up, basketball has always been a huge part of life. I loved playing basketball as well as watching NBA basketball. Whether I am on the court myself playing or watching my favorite player playing on TV, I was always engaged and interested in keeping up with the different statistics involved in the game of basketball.*
```{R}
library(tidyverse)
# read your datasets in here, e.g., with read_csv()
nbastats2018_2019 <- read_csv("nbastats2018-2019.csv", 
     col_types = cols(Height = col_number(), 
         Weight = col_number(), Age = col_number(), 
         Salary = col_number(), Points = col_number(), 
         Blocks = col_number(), Steals = col_number(), 
         Assists = col_number(), Rebounds = col_number(), 
         `FT%` = col_number(), `FG3%` = col_number(), 
         `FG%` = col_number(), MP = col_number(), 
         G = col_number()))
nbastats2018_2019
# if your dataset needs tidying, do so here

# any other code here

nbastats2018_2019 %>% filter(Citzenship == "USA")
nbastats2018_2019 %>% filter(Citzenship == "International")
```

### Cluster Analysis

```{R}
library(cluster)
# clustering code here
nbastats <-(na.omit(nbastats2018_2019))
subset(nbastats, select = -c(Name, Height, Weight, Team, Citzenship))
nbaclusterdata <- subset(nbastats, select = -c(Name, Height, Weight, Team, Citzenship))
sil_width <- vector()
for (i in 2:10) {
  kms <- kmeans(nbaclusterdata, centers = i)
  sil <- silhouette(kms$cluster, dist(nbaclusterdata))
  sil_width[i] <- mean(sil[, 3])
}
ggplot() + geom_line(aes(x = 1:10, y = sil_width)) + 
  scale_x_continuous(name = "k", breaks = 1:10)
nba_pam <- nbaclusterdata %>% pam(k = 2)
nba_pam
nba_pam$silinfo$avg.width
nbastats[23,]
nbastats[40,]
library(GGally)
nbaclusterdata %>% mutate(cluster = as.factor(nba_pam$clustering)) %>% 
    ggpairs(columns = c("Points", "Blocks", 
        "Steals", "Assists", "Rebounds", "Age", "cluster"), 
        aes(color = cluster))
nbaclusterdata %>% slice(nba_pam$id.med)
plot(nba_pam, which = 2)
```

Discussion of clustering here

*For this portion of the project, a new data called "nbastats" was created where it omitted all observations that included NAs. In the nbastats dataset, all the character columns such as Names, Teams, etc. and numeric variables height and weight were removed in order to move on with the PAM cluster analysis and this dataset was named "nbaclusterdata". The best number to use for a PAM clustering algorithm was k=2, and that was when the silhouette width was highest, at a value of around 0.67, which is the same as the overall average silhouette width. This number indicates that a reasonable structure has been found. Since all the names of the players were removed in the nba_pam data, I was not able to directly figure the names of the players directly. However, the user ID information was displayed from nba_pam and it was used in the "nbastats" data in order to find the names of the medoids. The two provinces that are the medoids were Jrue Holiday and Ben Simmons.*   
    
### Dimensionality Reduction with PCA

```{R}
# PCA code here
nbaclusterdata[c("Points", "Rebounds", "Assists")]
nbapca <- nbaclusterdata[c("Points", "Rebounds", "Assists")]
pca1 <- princomp(nbapca, cor = T)
summary(pca1, loadings = T)
library(factoextra)
fviz_pca_biplot(pca1)

```

Discussions of PCA here. 

*The "nbaclusterdata" was selected for PCA, but only the variables points, rebounds, and assists were selected. PC1 shows how if you score high on points and rebounds, there is a tradeoff of doing poorly on rebounds. PC2 shows how if you score high in points and rebounds, there is a tradeoff of doing poorly on assists. Lastly, PC3 shows how if you perform well with points, there is a tradeoff of doing poorly on rebounds and assists. This make sense because in the NBA, usually players who play the smaller positions such as guards usually score high in points and assists, but they do not grab as much rebounds. Similarly, NBA players who are taller and play the center position, they do well in grabbing rebounds and scoring points, but they do not do as well in the assists category. PC1 accounts for about 41 percent of the total variability, PC2 accounts for about 38 percent of the total variability, and PC3 accounts for about 21 percent of the total variability. The observations for the PC scores were displayed with a biplot through the fviz_pca function.*

###  Linear Classifier

```{R}
# linear classifier code here
logistic_fit <- glm(Citzenship == "USA" ~ Salary + Weight + Points + Blocks + Steals + Assists + Rebounds + Age + MP + G, data = nbastats, 
    family = "binomial")
prob_reg <- predict(logistic_fit, type = "response")
class_diag(prob_reg, nbastats$Citzenship, positive = "USA")
y<-nbastats$Citzenship
y<- factor(y, levels=c("International","USA"))
y_hat <- sample(c("International","USA"), size=length(y), replace=T)
y_hat <- factor(y_hat, levels=c("International","USA"))
table(actual = y, predicted = y_hat)

```

```{R}

set.seed(322)

k = 10

data <- sample_frac(nbastats)  
folds <- rep(1:k, length.out = nrow(data))  

diags <- NULL

i = 1
for (i in 1:k) {
    train <- data[folds != i, ]
    test <- data[folds == i, ]
    truth <- test$Citzenship
    
    
    fit <- glm(Citzenship == "USA" ~ Salary + Weight + Points + Blocks + Steals + Assists + Rebounds + Age + MP + G, data = test, 
        family = "binomial")  
    
    
    probs <- predict(fit, type = "response")  
    
   
    diags <- rbind(diags, class_diag(probs, truth, positive = "USA"))
}


summarize_all(diags, mean)
```

*The binary variable "citzenship" was predicted from the selected 10 numeric variables. The logistic regression method was utilized. In terms of how well the model was performing per AUC, the value was 0.9249, which is a pretty good value. Next, a confusion matrix was created. According to the confusion matrix, the values were displayed for false negative, true positive, true negative, and false positive in the top left corner, top right corner, bottom left corner, and borrom right corner, respectively. After doing a k-fold CV on this same model, the AUC value came out to be 1 this time. Overall, there seems to be no overfitting because the CV AUC looked comparable to the AUC value prior to the k-fold.*    

### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here
knn_fit <- knn3(Citzenship == "USA" ~ Salary + Weight + Points + Blocks + Steals + Assists + Rebounds + Age + MP + G, data = nbastats)
probs_knn <- predict(knn_fit, nbastats)[, 2]
class_diag(probs_knn, nbastats$Citzenship, positive = "USA")
y<-nbastats$Citzenship
y<- factor(y, levels=c("International","USA"))
y_hat <- sample(c("International","USA"), size=length(y), replace=T)
y_hat <- factor(y_hat, levels=c("International","USA"))
table(actual = y, predicted = y_hat)
```

```{R}
# cross-validation of np classifier here
set.seed(322)
k = 10

data <- sample_frac(nbastats)  
folds <- rep(1:k, length.out = nrow(data))  

diags <- NULL

i = 1
for (i in 1:k) {
    train <- data[folds != i, ]
    test <- data[folds == i, ]
    truth <- test$Citzenship
    
    
    fit <- knn3(Citzenship == "True" ~ Salary + Weight + Points + Blocks + Steals + Assists + Rebounds + Age + MP + G, data = test)  
    
    
    probs <- predict(fit,newdata = test)[, 1] 
    
    
    diags <- rbind(diags, class_diag(probs, truth, positive = "USA"))
}

summarize_all(diags, mean)
```

Discussion

*This time, a non-parametric classifier was used (k-nearest-neighbors) to the exact same dataset/variables that I used with the linear classifier above. The AUC values have decreased significantly when utilizing kNN. The AUC values were 0.77 prior to the k-fold and 0.55 after doing a k-fold to the same model. The decrease in AUC values indicates that there was overfitting. Overall,the logistic regression model performed the best per AUC.*  

### Regression/Numeric Prediction

```{R}
# regression model code here
linearfit<-lm(Points~.,data=nbaclusterdata) 
linearyhat<-predict(linearfit) 
mean((nbaclusterdata$Points-linearyhat)^2)
```

```{R}
# cross-validation of regression model here
set.seed(1234)
k=5 
data<-nbaclusterdata[sample(nrow(nbaclusterdata)),] 
folds<-cut(seq(1:nrow(nbaclusterdata)),breaks=k,labels=F) 
diags<-NULL
set.seed(1234)
k=5 
data<-nbaclusterdata[sample(nrow(nbaclusterdata)),] 
folds<-cut(seq(1:nrow(nbaclusterdata)),breaks=k,labels=F) 
diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  
  linearfit<-lm(Points~.,data=train)
 
  linearyhat<-predict(linearfit,newdata=test)
 
  diags<-mean((test$Points-linearyhat)^2)
}
mean(diags)  
```

Discussion

*In this section, a linear regression model was performed for the nbaclusterdata dataset because the original nbastats dataset was running into errors. I believe it was due to the fact that the original the nbastats data involved character variables such as the names of the NBA players. The points variable was being predicted from all other variables. The MSE value came out to be 8.50655 which is fairly a low measure of prediction error. Next, the average MSE across the k testing folds came out to be 5.050184. It can be concluded that the MSE was lower in CV, which is a good sign (means no overfitting).*

### Python 

```{R}
library(reticulate)
use_python("/usr/bin/python3", required = F)
testing<-"Basketball is"

```

```{python}
# python code here
testing="my favorite sport."
print(r.testing,testing) 
```
```{R}
testing<-"Basketball is"
cat(c(testing,py$testing)) 
```
Discussion

*Utilizing the reticualte package, I was able to utilize python codes and r codes all on this R Markdown. Just as we have been doing all semester, I have created a character vector under the {R} code section called "testing" which contains the text "Basketball is". Additionally, I did the same thing under the {python} code section except the code "testing" contained the text "my favorite sport". Despite having the same code name "testing", utilizing the r. code under {python} section and py$ code under the {R} section allowed me to print out the phrase "Basketball is my favorite sport". This method allowed me to have objects be named the same thing without having any conflicts. The code r. allowed me to access R-Defined objects under the python code chunk area and the code py$ allowed me to access python-defined objects under the r code chunk area.*   

### Concluding Remarks

Include concluding remarks here, if any




